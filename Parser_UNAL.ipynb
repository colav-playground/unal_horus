{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6d99f-44f1-4481-8b8c-3b8f6ef544ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V4\n",
    "import pandas as pd\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from kahi_impactu_utils.Utils import doi_processor\n",
    "from unidecode import unidecode\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# MongoDB Configuration\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"  # Replace with your MongoDB URI\n",
    "DB_NAME = \"kahi_dev\"\n",
    "COLLECTION_NAME = \"person\"\n",
    "DB_OPENALEX = \"openalex\"\n",
    "OPENALEX_COLLECTION_NAME = \"works\"\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def read_csv_file(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def extract_works_dois(filtered_works):\n",
    "    works_dois = [row['Doi'] for _, row in filtered_works.iterrows() if doi_processor(row['Doi'])]\n",
    "    return works_dois\n",
    "\n",
    "def analyze_works(works_dois, mongo_uri, db_name, collection_name):\n",
    "    with MongoClient(mongo_uri) as client:\n",
    "        db = client[db_name]\n",
    "        collection = db[collection_name]\n",
    "        works_authorships = []\n",
    "\n",
    "        for doi in works_dois:\n",
    "            work_db = collection.find_one({\"doi\": \"https://doi.org/\"+doi}, {\"authorships.author\"})\n",
    "            if work_db:\n",
    "                works_authorships.append({\"doi\": doi, \"authorships\": work_db[\"authorships\"]})\n",
    "\n",
    "    return works_authorships\n",
    "\n",
    "def get_author_identifiers(authors_dicts, works_authorships):\n",
    "    return\n",
    "\n",
    "def create_authors_dicts(authors_list):\n",
    "    author_identifiers = []\n",
    "    for author in authors_list:\n",
    "        entry = {\n",
    "            \"author_full_name\": author,\n",
    "            \"primer_apellido\": \"\",\n",
    "            \"segundo_apellido\": \"\",\n",
    "            \"nombres\": \"\",\n",
    "            \"author_full_name_normalized\": unidecode(author.lower()),\n",
    "            \"author_ids\": [],\n",
    "        }\n",
    "        author_identifiers.append(entry)\n",
    "    return author_identifiers\n",
    "\n",
    "# Main Script\n",
    "def main():\n",
    "    # File paths\n",
    "    excel_file_path = \"UNAL_13_01_2025.xlsx\"\n",
    "    csv_file_path = \"Lista de docentes investigadores.csv\"\n",
    "    \n",
    "    works_authorships = []\n",
    "\n",
    "    # Read Excel file\n",
    "    data = read_excel_file(excel_file_path)\n",
    "\n",
    "    # Filter rows with valid DOI\n",
    "    filtered_data = data[data['Doi'].notnull()]\n",
    "\n",
    "    # Read CSV file\n",
    "    csv_authors = read_csv_file(csv_file_path)\n",
    "\n",
    "    authors_dicts = create_authors_dicts(csv_authors['Nombre'].tolist())\n",
    "    print(f'{len(authors_dicts)} authors have been extracted\\n')\n",
    "\n",
    "    #works_dois = extract_works_dois(data)\n",
    "    #print(f'{len(works_dois)} DOI have been extracted\\n')\n",
    "\n",
    "    #works_authorships = analyze_works(works_dois, MONGO_URI, DB_OPENALEX, OPENALEX_COLLECTION_NAME)\n",
    "\n",
    "    #get_author_identifiers(authors_dicts, works_authorships)\n",
    "    works_authorships = []\n",
    "    #get_author_identifiers(authors_dicts)\n",
    "\n",
    "    # Output result\n",
    "    return authors_dicts, works_authorships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6619ce69-6e42-42fb-9d66-608ac46e3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "authors_dicts, works_authorships = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb6a8a-9c24-4a7d-ae2b-013acbb72c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbclient[\"parser_unal\"]\n",
    "authors_collection = db[\"authors\"]\n",
    "works_collection = db[\"works\"]\n",
    "\n",
    "authors_collection.insert_many(authors_dicts)\n",
    "#works_collection.insert_many(works_authorships)\n",
    "\n",
    "authors_collection.count_documents({}), works_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4314d7f-8efa-45ea-b9c3-2a1be09e45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "json_file_path = \"unal_authors.json\"\n",
    "\n",
    "# Read json file\n",
    "json_authors = read_json_file(json_file_path)\n",
    "\n",
    "def process_one_author(author_doc):\n",
    "    normalized_name_1 = author_doc[\"author_full_name_normalized\"]\n",
    "    set1 = set(normalized_name_1.split())\n",
    "\n",
    "    for author in json_authors:\n",
    "        display_name = author[\"full_name\"]\n",
    "        normalized_name_2 = unidecode(display_name.lower())\n",
    "        set2 = set(normalized_name_2.split())\n",
    "\n",
    "        if not set1.difference(set2) and not set2.difference(set1):\n",
    "            # Evita duplicados en 'author_ids'\n",
    "            author_doc[\"primer_apellido\"] = author[\"last_names\"][0],\n",
    "            author_doc[\"segundo_apellido\"] = author[\"last_names\"][1] if len(author[\"last_names\"]) > 1 else \"\",\n",
    "            author_doc[\"nombres\"] = author[\"first_names\"]\n",
    "            cod_rh = author[\"external_ids\"][\"id\"][\"COD_RH\"]\n",
    "            rh_id = {\"source\": \"scienti\", \"id\": cod_rh}\n",
    "            if rh_id not in author_doc[\"author_ids\"]:\n",
    "                author_doc[\"author_ids\"].append(rh_id)\n",
    "                #print(cod_rh)\n",
    "\n",
    "    authors_collection.update_one(\n",
    "    {\"_id\": author_doc[\"_id\"]},\n",
    "    {\"$set\": {\n",
    "        \"primer_apellido\": author_doc[\"primer_apellido\"],\n",
    "        \"segundo_apellido\": author_doc[\"segundo_apellido\"],\n",
    "        \"nombres\": author_doc[\"nombres\"],\n",
    "        \"author_ids\": author_doc[\"author_ids\"]\n",
    "    }}\n",
    ")\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# All authors from json file\n",
    "db = client[\"parser_unal\"]\n",
    "authors_collection = db[\"authors\"]\n",
    "authors_list = list(authors_collection.find({}))\n",
    "Parallel(\n",
    "    n_jobs=72,\n",
    "    verbose=5,\n",
    "    backend=\"multiprocessing\",\n",
    ")(\n",
    "    delayed(process_one_author)(author) for author in authors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcada0-f2e0-47e5-99d3-66822e114ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All authors from JSON file - Partial Name with one name\n",
    "from collections import Counter\n",
    "\n",
    "def extract_remaining_name(full_name, first_lastname, second_lastname, names):\n",
    "    full_name_parts = full_name.split()\n",
    "    for lastname in first_lastname:\n",
    "        if unidecode(lastname) in full_name_parts:\n",
    "            full_name_parts.remove(unidecode(lastname))\n",
    "    for lastname in second_lastname:\n",
    "        if unidecode(lastname) in full_name_parts:\n",
    "            full_name_parts.remove(unidecode(lastname))     \n",
    "    for name in names:\n",
    "        if unidecode(name) in full_name_parts:\n",
    "            full_name_parts.remove(unidecode(name))\n",
    "    return full_name_parts if full_name_parts else None\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "json_file_path = \"unal_authors.json\"\n",
    "\n",
    "# Read json file\n",
    "json_authors = read_json_file(json_file_path)\n",
    "\n",
    "def process_one_author(author_doc):\n",
    "    normalized_name_1 = author_doc[\"author_full_name_normalized\"]\n",
    "    normalized_name_1_split = normalized_name_1.split()\n",
    "    #set1 = set(normalized_name_1_split)\n",
    "\n",
    "    for author in json_authors:\n",
    "        display_name = author[\"full_name\"]\n",
    "        normalized_name_2 = unidecode(display_name.lower())\n",
    "        normalized_name_2_split = normalized_name_2.split()\n",
    "        #set2 = set(normalized_name_2_split)\n",
    "\n",
    "        if not list((Counter(normalized_name_2_split) - Counter(normalized_name_1_split)).elements()):\n",
    "            # Evita duplicados en 'author_ids'\n",
    "            author_doc[\"primer_apellido\"] = [author[\"last_names\"][0].strip()]\n",
    "            author_doc[\"segundo_apellido\"] = [author[\"last_names\"][1].strip()] if len(author[\"last_names\"]) > 1 else []\n",
    "            author_doc[\"nombres\"] = author[\"first_names\"]\n",
    "\n",
    "            if author_doc[\"primer_apellido\"] and author_doc[\"segundo_apellido\"] and author_doc[\"nombres\"]:\n",
    "                remaining_name = extract_remaining_name(author_doc[\"author_full_name\"], author_doc[\"primer_apellido\"], author_doc[\"segundo_apellido\"], author_doc[\"nombres\"])\n",
    "                if remaining_name:\n",
    "                    for name in remaining_name:\n",
    "                        author_doc[\"nombres\"] = list(author_doc[\"nombres\"])\n",
    "                        author_doc[\"nombres\"].append(name)\n",
    "\n",
    "            cod_rh = author[\"external_ids\"][\"id\"][\"COD_RH\"]\n",
    "            rh_id = {\"source\": \"scienti\", \"id\": cod_rh}\n",
    "            if rh_id not in author_doc[\"author_ids\"]:\n",
    "                author_doc[\"author_ids\"].append(rh_id)\n",
    "                #print(cod_rh)\n",
    "\n",
    "    authors_collection.update_one(\n",
    "    {\"_id\": author_doc[\"_id\"]},\n",
    "    {\"$set\": {\n",
    "        \"primer_apellido\": author_doc[\"primer_apellido\"],\n",
    "        \"segundo_apellido\": author_doc[\"segundo_apellido\"],\n",
    "        \"nombres\": author_doc[\"nombres\"],\n",
    "        \"author_ids\": author_doc[\"author_ids\"]\n",
    "    }}\n",
    ")\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# All authors from json file\n",
    "db = client[\"parser_unal\"]\n",
    "authors_collection = db[\"authors\"]\n",
    "authors_list = list(authors_collection.find({\"author_ids\": {\"$eq\": []}}))\n",
    "Parallel(\n",
    "    n_jobs=72,\n",
    "    verbose=5,\n",
    "    backend=\"multiprocessing\",\n",
    ")(\n",
    "    delayed(process_one_author)(author) for author in authors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fec85b-aa8c-4cf1-8bc7-5952643147c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors without ids vs authors with scienti ids\n",
    "def process_one_author(author_doc):\n",
    "    normalized_name_1 = unidecode(author_doc[\"author_full_name\"].lower())\n",
    "    set1 = set(normalized_name_1.split())\n",
    "\n",
    "    for min_auth in db_authors_minciencias:\n",
    "        display_name = min_auth[\"full_name\"]\n",
    "        normalized_name_2 = unidecode(display_name.lower())\n",
    "        set2 = set(normalized_name_2.split())\n",
    "    \n",
    "        if not set1.difference(set2) and not set2.difference(set1):\n",
    "            # Evita duplicados en 'author_ids'\n",
    "            author_doc[\"primer_apellido\"] = min_auth[\"last_names\"][0],\n",
    "            author_doc[\"segundo_apellido\"] = min_auth[\"last_names\"][1] if len(min_auth[\"last_names\"]) > 1 else \"\",\n",
    "            author_doc[\"nombres\"] = \" \".join(min_auth[\"first_names\"]),\n",
    "    \n",
    "            cod_rh = next((exid[\"id\"][\"COD_RH\"] for exid in min_auth[\"external_ids\"] if exid[\"source\"] == \"scienti\" or exid[\"source\"] == \"minciencias\"), \"\")\n",
    "            if cod_rh:\n",
    "                rh_id = {\"source\": \"scienti\", \"id\": cod_rh} \n",
    "                if rh_id not in author_doc[\"author_ids\"]:\n",
    "                    author_doc[\"author_ids\"].append(rh_id)\n",
    "    \n",
    "    authors_collection.update_one(\n",
    "        {\"_id\": author_doc[\"_id\"]},\n",
    "        {\"$set\": {\n",
    "            \"primer_apellido\": author_doc[\"primer_apellido\"],\n",
    "            \"segundo_apellido\": author_doc[\"segundo_apellido\"],\n",
    "            \"nombres\": author_doc[\"nombres\"],\n",
    "            \"author_ids\": author_doc[\"author_ids\"]\n",
    "        }}\n",
    "    )\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# Authors without ids vs authors with scienti ids\n",
    "db = client[\"parser_unal\"]\n",
    "authors_collection = db[\"authors\"]\n",
    "authors_list = list(authors_collection.find({\"author_ids\": {\"$eq\": []}}))\n",
    "db_authors_minciencias = list(client[\"kahi_dev_stable\"][\"person\"].find({\"external_ids.source\": \"scienti\"}, {\"full_name\", \"first_names\", \"last_names\", \"external_ids\"}))\n",
    "\n",
    "Parallel(\n",
    "    n_jobs=72,\n",
    "    verbose=5,\n",
    "    backend=\"multiprocessing\",\n",
    ")(\n",
    "    delayed(process_one_author)(author) for author in authors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b5b8f-277f-469e-a225-3238937d34fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20399803-3538-4613-8f1d-a208a3bc08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar solo los trabajos de autores de los cuales se asocio COD_RH\n",
    "authors_list = list(authors_collection.find({\"author_ids\": {\"$ne\": []}}))\n",
    "valid_authors = [author[\"author_full_name\"] for author in authors_list]\n",
    "source_file = \"/home/fvergara/projects/colav/data/UNAL/UNAL_13_01_2025.xlsx\"\n",
    "df = pd.read_excel(source_file)\n",
    "\n",
    "df_filtered = df[df[\"Coautores\"].astype(str).apply(lambda x: any(author in x.split(\", \") for author in valid_authors))]\n",
    "\n",
    "output_file = \"UNAL_13_01_2025_filtered.xlsx\"\n",
    "df_filtered.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfa60f-43fe-4abf-b8ce-8d61963efe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97affc63-0275-4e6c-849f-b981ed285d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear coleccion de mongodb con los registros de los trabajos\n",
    "from kahi_impactu_utils.Utils import doi_processor\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def read_excel(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "def get_mongo_client(uri=\"mongodb://localhost:27017/\"):\n",
    "    client = MongoClient(uri)\n",
    "    return client\n",
    "\n",
    "def search_author(author, external_authors_collection):\n",
    "    return external_authors_collection.find_one({\"author_full_name\": author})\n",
    "\n",
    "def insert_staff_record(staff_collection, author_data, cod_rh):\n",
    "    # Se verifica que no exista un autor con el mismo identificador\n",
    "    if staff_collection.find_one({\"identificación\": cod_rh}):\n",
    "        # Ya existe: no se inserta de nuevo\n",
    "        return\n",
    "\n",
    "    # Se arma el registro de STAFF. Se utiliza el campo \"author_full_name\" para almacenar el nombre completo.\n",
    "    staff_record = {\n",
    "        # \"author_full_name\": author_data.get(\"author_full_name\", \"\"),\n",
    "        \"tipo_documento\": \"COD_RH\",\n",
    "        \"identificación\": cod_rh,\n",
    "        # Se asume que en author_data los apellidos y nombres pueden venir como lista, por lo que se hace join\n",
    "        \"primer_apellido\": \" \".join(author_data.get(\"primer_apellido\", [])) if isinstance(author_data.get(\"primer_apellido\"), list) else author_data.get(\"primer_apellido\", \"\"),\n",
    "        \"segundo_apellido\": \" \".join(author_data.get(\"segundo_apellido\", [])) if isinstance(author_data.get(\"segundo_apellido\"), list) else author_data.get(\"segundo_apellido\", \"\"),\n",
    "        \"nombres\": \" \".join(author_data.get(\"nombres\", [])) if isinstance(author_data.get(\"nombres\"), list) else author_data.get(\"nombres\", \"\"),\n",
    "        \"nivel_académico\": \"\",\n",
    "        \"tipo_contrato\": \"\",\n",
    "        \"jornada_laboral\": \"\",\n",
    "        \"categoría_laboral\": \"\",\n",
    "        \"sexo\": \"\",\n",
    "        \"fecha_nacimiento\": \"\",\n",
    "        \"fecha_inicial_vinculación\": \"\",\n",
    "        \"fecha_final_vinculación\": \"\",\n",
    "        \"código_unidad_académica\": \"\",\n",
    "        \"unidad_académica\": \"\",\n",
    "        \"código_subunidad_académica\": \"\",\n",
    "        \"subunidad_académica\": \"\",\n",
    "    }\n",
    "    staff_collection.insert_one(staff_record)\n",
    "\n",
    "def insert_ciarp_record(ciarp_collection, row, cod_rh):\n",
    "    # Se obtiene el año a partir del campo Fecha (formato \"%d/%m/%Y\")\n",
    "    try:\n",
    "        date_obj = datetime.datetime.strptime(row[\"Fecha\"], \"%d/%m/%Y\")\n",
    "        year_str = str(date_obj.year)\n",
    "    except Exception:\n",
    "        year_str = \"\"\n",
    "\n",
    "    ciarp_record = {\n",
    "        \"código_unidad_académica\": \"\",\n",
    "        \"código_subunidad_académica\": \"\",\n",
    "        \"tipo_documento\": \"COD_RH\",\n",
    "        \"identificación\": cod_rh,\n",
    "        \"año\": year_str,\n",
    "        \"título\": row[\"Título original\"],\n",
    "        \"idioma\": row[\"Idioma\"],\n",
    "        \"revista\": row[\"Revista / Conferencia\"],\n",
    "        \"editorial\": \"\",\n",
    "        \"doi\": row[\"Doi\"],  # Puedes optar por usar el DOI procesado (variable doi) si lo deseas.\n",
    "        \"issn\": row[\"ISSN\"],\n",
    "        \"isbn\": row[\"ISBN\"],\n",
    "        \"volumen\": \"\",\n",
    "        \"issue\": \"\",\n",
    "        \"primera_página\": \"\",\n",
    "        \"pais_producto\": \"\",\n",
    "        \"última_página\": \"\",\n",
    "        \"entidad_premiadora\": \"\",\n",
    "        \"ranking\": row[\"Tipo\"],\n",
    "    }\n",
    "    ciarp_collection.insert_one(ciarp_record)\n",
    "\n",
    "def process_data_and_insert(dataframe, external_authors_collection, staff_collection, ciarp_collection):\n",
    "    for _, row in dataframe.iterrows():\n",
    "        # Procesa el DOI (si es una cadena válida)\n",
    "        doi = \"\"\n",
    "        if isinstance(row[\"Doi\"], str):\n",
    "            doi = doi_processor(row[\"Doi\"])\n",
    "            # Actualiza el valor del DOI en la fila si se requiere:\n",
    "            row[\"Doi\"] = doi\n",
    "\n",
    "        # Separa los coautores (asumiendo que vienen separados por \", \")\n",
    "        coauthors = str(row[\"Coautores\"]).split(\", \")\n",
    "\n",
    "        for author in coauthors:\n",
    "            # Busca los datos completos del autor en la colección externa\n",
    "            author_data = search_author(author, external_authors_collection)\n",
    "            if not author_data:\n",
    "                # Si el autor no existe en la fuente externa, se omite este registro\n",
    "                continue\n",
    "\n",
    "            # Se extrae el código RH (identificador) a partir de la lista de ids\n",
    "            cod_rh = next((exid[\"id\"] for exid in author_data.get(\"author_ids\", []) if \"id\" in exid), \"\")\n",
    "            if not cod_rh:\n",
    "                continue\n",
    "\n",
    "            # Inserta en la colección staff si aún no existe\n",
    "            insert_staff_record(staff_collection, author_data, cod_rh)\n",
    "            # Inserta el registro de producción en la colección ciarp\n",
    "            insert_ciarp_record(ciarp_collection, row, cod_rh)\n",
    "\n",
    "def main(input_file):\n",
    "    # Leer el archivo Excel\n",
    "    data = read_excel(input_file)\n",
    "    print(f\"Source Dataframe shape: {data.shape}\")\n",
    "\n",
    "    # Conectar a MongoDB y definir la base de datos y colecciones\n",
    "    client = get_mongo_client(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"parser_unal\"]\n",
    "\n",
    "    # Se asume que en la base de datos existe una colección con los datos completos de autores.\n",
    "    authors_collection = db[\"authors\"]\n",
    "\n",
    "    # Estas serán las colecciones donde se insertarán los registros procesados.\n",
    "    staff_collection = db[\"staff\"]\n",
    "    ciarp_collection = db[\"ciarp\"]\n",
    "\n",
    "    # Procesar la información e insertar en las colecciones\n",
    "    process_data_and_insert(data, authors_collection, staff_collection, ciarp_collection)\n",
    "\n",
    "    print(\"Processing complete. Data inserted into MongoDB collections 'staff' and 'ciarp'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c90b28-4254-411e-9633-2021e7af8084",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main(\"UNAL_13_01_2025_filtered.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c95e21-3e05-46c8-807f-555e48e75047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac13fb-3fbf-47c1-b173-c532cab78478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear STAFF y CIARP\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def get_mongo_client(uri=\"mongodb://localhost:27017/\"):\n",
    "    client = MongoClient(uri)\n",
    "    return client\n",
    "\n",
    "def export_collections_to_excel(uri=\"mongodb://localhost:27017/\", db_name=\"my_database\",\n",
    "                                staff_filename=\"formato_talento_humano_unal_2025_02.xlsx\", ciarp_filename=\"formato_CIARP_UNAL_2025_02.xlsx\"):\n",
    "    # Conectar a MongoDB y seleccionar la base de datos\n",
    "    client = get_mongo_client(uri)\n",
    "    db = client[db_name]\n",
    "    \n",
    "    # Seleccionar las colecciones\n",
    "    staff_collection = db[\"staff\"]\n",
    "    ciarp_collection = db[\"ciarp\"]\n",
    "\n",
    "    # Leer todos los documentos de la colección staff\n",
    "    staff_docs = list(staff_collection.find({}))\n",
    "    if staff_docs:\n",
    "        staff_df = pd.DataFrame(staff_docs)\n",
    "        if \"_id\" in staff_df.columns:\n",
    "            staff_df.drop(columns=[\"_id\"], inplace=True)\n",
    "        if \"author_full_name\" in staff_df.columns:\n",
    "            staff_df.drop(columns=[\"author_full_name\"], inplace=True)\n",
    "        # Guardar el DataFrame en un archivo Excel\n",
    "        staff_df.to_excel(staff_filename, index=False)\n",
    "        print(f\"Archivo generado: {staff_filename}\")\n",
    "    else:\n",
    "        print(\"La colección 'staff' está vacía.\")\n",
    "\n",
    "    # Leer todos los documentos de la colección ciarp\n",
    "    ciarp_docs = list(ciarp_collection.find({}))\n",
    "    if ciarp_docs:\n",
    "        ciarp_df = pd.DataFrame(ciarp_docs)\n",
    "        if \"_id\" in ciarp_df.columns:\n",
    "            ciarp_df.drop(columns=[\"_id\"], inplace=True)\n",
    "        ciarp_df.to_excel(ciarp_filename, index=False)\n",
    "        print(f\"Archivo generado: {ciarp_filename}\")\n",
    "    else:\n",
    "        print(\"La colección 'ciarp' está vacía.\")\n",
    "\n",
    "# Puedes ajustar la URI, el nombre de la base de datos y los nombres de archivos según tu entorno.\n",
    "export_collections_to_excel(uri=\"mongodb://localhost:27017/\", db_name=\"parser_unal\",\n",
    "                            staff_filename=\"formato_talento_humano_unal_2025_02.xlsx\", ciarp_filename=\"formato_CIARP_UNAL_2025_02.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
